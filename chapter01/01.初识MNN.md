
# 1. MNN 轻量级高性能推理引擎
![](https://easyimage.elyt.cn/i/2025/05/11/5733293741809079105-2.webp)  


[MNN](https://www.mnn.zone/) 是一个轻量级高性能推理引擎，由阿里巴巴团队开发，支持Android、iOS、Windows、Linux、MacOS、WebGL等平台。
特点如下：
- 通用性 - 支持TensorFlow、Caffe、ONNX等主流模型格式，支持- CNN、RNN、GAN等常用网络。
- 高性能 - 极致优化算子性能，全面支持CPU、GPU、NPU，充分发挥设备算力。
- 易用性 - 转换、可视化、调试工具齐全，能方便地部署到移动设备和各种嵌入式设备中。


## 1.1. 架构
![](https://easyimage.elyt.cn/i/2025/05/11/5733290806928742935-2.webp)  



### 1.1.1. GPU 

#### 1.1.1.1. OPENCL 介绍 
OpenCL（Open Computing Language）和OpenGL（Open Graphics Library）都是由Khronos Group维护的开放标准，但它们的设计目的和应用场景有所不同。

##### 1.1.1.1.1. OpenCL

- **用途**: OpenCL是一个用于编写跨平台并行计算程序的框架。它不仅支持图形处理，还广泛应用于科学计算、机器学习、图像处理、信号处理等领域。
- **适用范围**: 适用于多种类型的计算设备，包括CPU、GPU、FPGA和其他加速器。它提供了一个编程模型，使开发者能够编写出可以利用所有可用计算资源的应用程序。
- **编程模型**: OpenCL采用C语言的一个子集作为其内核编程语言，并允许开发者手动管理内存和控制执行流程，从而提供了高度的灵活性和控制力。

##### 1.1.1.1.2. OpenGL

- **用途**: OpenGL主要用于渲染2D和3D图形。它提供了一系列API，让开发者可以创建高性能的图形应用程序，如视频游戏、模拟器、虚拟现实应用等。
- **适用范围**: 主要针对图形处理单元（GPU），专注于图形渲染任务。虽然现代OpenGL也支持一些通用计算功能（通过OpenGL的compute shaders），但它的主要关注点仍然是图形渲染。
- **编程模型**: OpenGL不是一个编程语言，而是一套API，用于与图形硬件进行交互。它负责处理渲染管道的各个方面，从几何形状的转换到像素颜色的计算等。

**总结**

尽管两者都可以在GPU上运行，但它们的目的不同：OpenCL是为了实现并行计算任务的高效执行，而OpenGL则是为了生成高质量的2D和3D图形。对于需要同时利用GPU进行图形渲染和通用计算的应用程序，可能会同时使用OpenGL和OpenCL来分别满足图形和计算的需求。然而，在某些情况下，例如现代游戏开发中，开发者可能更倾向于使用更高层次的游戏引擎或API，这些引擎/API内部可能已经整合了对OpenGL和OpenCL的支持。


#### 1.1.1.2. Vulkan 介绍

Vulkan 是由Khronos Group开发和维护的一个低开销、跨平台的2D和3D图形及计算API。它旨在提供高性能和更直接的控制硬件资源，从而允许开发者更好地利用现代GPU的能力，特别是在并行处理和多线程编程方面。

##### 1.1.1.2.1. 主要特点

- **低开销**：与前一代图形API（如OpenGL）相比，Vulkan通过减少驱动程序对应用程序的干预来降低CPU的开销。这意味着更多的控制权在开发者手中，但也要求开发者承担更多责任，例如手动管理内存和同步操作。
  
- **跨平台支持**：Vulkan被设计为可以在多种操作系统上运行，包括Windows、Linux、Android以及macOS（通过MoltenVK，一个将Vulkan API转换为Metal API的库）。这使得开发者可以编写一次代码并在多个平台上运行。
  
- **多线程友好**：Vulkan的设计考虑到了多线程环境下的高效使用，允许工作负载在多个核心上并行执行，从而提高性能。
  
- **显式控制**：Vulkan提供了对渲染管道的显式控制，包括资源管理和同步等。虽然这增加了复杂性，但同时也给予了开发者优化应用性能的机会。

##### 1.1.1.2.2. 应用场景

Vulkan适用于需要高效率和高性能图形或计算的应用场景，如视频游戏、虚拟现实(VR)、增强现实(AR)、科学计算、机器学习等。由于其高效的特性和对多线程的支持，对于那些寻求在高端PC、移动设备或者控制台上的最佳性能的开发者来说，Vulkan是一个理想的选择。

#### 1.1.1.3. CUDA 


CUDA（Compute Unified Device Architecture，计算统一设备架构）是由NVIDIA公司开发的一个并行计算平台和应用程序编程接口（API）。它允许开发者利用NVIDIA的图形处理单元（GPU）进行通用计算任务，而不仅仅是传统的图形渲染。CUDA的设计目的是为了提高计算密集型应用的性能，通过利用GPU强大的并行处理能力来加速科学计算、机器学习、图像处理等多种领域的任务。

##### 1.1.1.3.1. 主要特点

- **并行计算**：CUDA使得开发者能够编写程序以充分利用GPU的高度并行结构。一个典型的GPU可能包含数千个核心，这使其在处理大规模并行计算任务时比传统CPU更加高效。
  
- **易用性**：CUDA使用类似于C++的编程语言，同时也支持其他语言如Fortran、Python等，通过相应的库或封装实现。这让熟悉这些语言的开发者能够相对容易地开始使用CUDA进行开发。
  
- **广泛的库支持**：CUDA提供了丰富的库支持，例如cuBLAS（用于基本线性代数子程序）、cuFFT（快速傅里叶变换）、cuDNN（深度神经网络）等，极大地简化了高性能应用的开发过程。
  
- **跨平台兼容性**：虽然CUDA主要是为NVIDIA GPU设计的，但它可以在多个操作系统上运行，包括Windows、Linux和macOS，确保了其广泛的适用性。

##### 1.1.1.3.2. 应用场景

CUDA被广泛应用于需要大量计算资源的领域，如：

- **科学计算**：包括物理模拟、数值分析、气象预报等。
- **机器学习与人工智能**：训练复杂的深度学习模型，尤其是在计算机视觉、自然语言处理等领域。
- **图像和视频处理**：实时图像处理、视频编码解码、特效生成等。
- **大数据分析**：加速数据处理和分析任务，如数据库查询优化、金融建模等。




#### 1.1.1.4. Metal 
Metal 是苹果公司推出的一个底层图形和计算API，专为iOS、macOS以及tvOS设计。它首次在2014年的WWDC（苹果全球开发者大会）上被介绍，并旨在为开发者提供对硬件资源的直接访问，以实现高性能的图形渲染和并行计算任务。

##### 1.1.1.4.1. 主要特点

- **低开销**：Metal 通过减少CPU的负担来优化性能，允许应用程序更高效地使用GPU资源。它减少了驱动程序的干预，提供了接近硬件层面的访问权限。
  
- **跨苹果平台支持**：虽然 Metal 主要是为苹果生态系统设计的，但它可以在多个苹果平台上使用，包括 iOS、iPadOS、macOS 和 tvOS，这使得开发者可以编写一次代码并在不同的苹果设备上运行。
  
- **高效的多线程处理能力**：Metal 支持并鼓励使用多线程技术，允许工作负载分散到多个CPU核心上，从而提高应用的整体性能。
  
- **精细控制**：Metal 提供了对图形管道和资源管理的精细控制，允许开发者根据自己的需求优化应用性能。例如，开发者可以直接管理资源状态转换、同步操作等。

##### 1.1.1.4.2. 应用场景

Metal 特别适用于需要高性能图形渲染和计算的应用，如3D游戏、虚拟现实(VR)、增强现实(AR)、图像和视频编辑软件等。由于其高度优化的特性和对苹果硬件的紧密集成，许多顶级游戏引擎和应用程序都采用了 Metal 来提升性能和效率。


## 1.2. MNN 整体特点

### 1.2.1. 轻量性

- 主体功能（模型推理CPU+GPU）无任何依赖，代码精简，可以方便地部署到移动设备和各种嵌入式设备中。
   - iOS平台：功能全开的MNN静态库 armv7+arm64大小12MB左右，链接生成可执行文件增加大小2M左右。可裁剪主体功能后静态库大小6.1M ，链接生成可执行文件增加大小 600 KB。
   - Android平台：主体功能 armv7a - c++_shared 动态库大小800KB左右。
- 支持采用 Mini 编辑选项进一步降低包大小，大约能在上述库体积基础上进一步降低 25% 左右。
- 支持模型FP16/Int8压缩与量化，可减少模型50% - 75% 的体积

### 1.2.2. 通用性

- 支持 Tensorflow、Caffe、ONNX、Torchscripts 等主流模型文件格式，支持CNN / RNN / GAN / Transformer 等主流网络结构。
- 支持多输入多输出，支持任意维度的输入输出，支持动态输入（输入大小可变），支持带控制流的模型
- 算子丰富，支持 178 个Tensorflow Op、52个 Caffe Op、163个 Torchscipts Op、158 个 ONNX Op（ONNX 基本完整支持）
- 支持 服务器 / 个人电脑 / 手机 及具有POSIX接口的嵌入式设备，支持使用设备的 CPU / GPU 计算，支持部分设备的 NPU 计算（IOS 11 + CoreML / Huawei + HIAI / Android + NNAPI）
- 支持 Windows / iOS 8.0+ / Android 4.3+ / Linux  及具有POSIX接口的操作系统

### 1.2.3. 高性能

- 对iOS / Android / PC / Server 的CPU架构进行了适配，编写SIMD代码或手写汇编以实现核心运算，充分发挥 CPU的算力，单线程下运行常见CV模型接近设备算力峰值
- 支持基于 Metal / OpenCL / Vulkan 使用移动端设备上的GPU进行推理
- 支持基于 CUDA 使用 PC / Server 上的 NVIDIA GPU 实现更快速的推理
- 广泛运用了 Winograd 卷积算法提升卷积性能，首次在业界工程实践中实现转置卷积的Winograd算法优化与矩阵乘的Strassen算法优化，并取得加速效果
- 支持低精度计算（ int8 / fp16 / bf16）以提升推理性能。并对 ARMv8.2 和 AVX512架构的相关指令进行了适配，这两种架构下有更好的加速效果

### 1.2.4. 易用性

- 支持使用 MNN 的算子进行常用的数值计算，覆盖 numpy 常用功能
- 提供 MNN CV 模块，支持图像仿射变换与归一化等 MNN_CV 库，支持常用的图像处理（armv7a 架构下小于 100 k ）
- 支持各平台下的模型训练，尤其是移动端上的模型训练
- 支持 python 调用

MNN适配的硬件架构与精度详见下表：

- S ：支持，深度优化并已有应用场景，推荐使用
- A ：支持，有初步优化或已有应用场景，可以使用
- B ：支持，无优化或在实验状态，不推荐使用
- C ：不支持

| Architecture / Precision |  | Normal | FP16 | BF16 | Int8 / Int4 |
| --- | --- | --- | --- | --- | --- |
| CPU | Native | B | C | B | B |
|  | x86/x64-SSE4.1 | A | B | B | A |
|  | x86/x64-AVX2 | S | B | B | A |
|  | x86/x64-AVX512 | S | B | B | S |
|  | ARMv7a | S | S (ARMv8.2) | S | S |
|  | ARMv8 | S | S (ARMv8.2) | S(ARMv8.6) | S |
| GPU | OpenCL | A | S | C | S |
|  | Vulkan | A | A | C | A |
|  | Metal | A | S | C | S |
|  | CUDA | A | S | C | A |
| NPU | CoreML | A | C | C | C |
|  | HIAI | A | C | C | C |
|  | NNAPI | B | B | C | B |


## 1.3. 工具

基于MNN (张量计算引擎)，提供了一系列工具，以支持模型推理、训练和通用计算：


- MNN-Converter：模型转换工具，由Frontends和Graph Optimize构成。前者负责支持不同的训练框架，MNN当前支持Tensorflow(Lite)、Caffe、ONNX(PyTorch/MXNet的模型可先转为ONNX模型再转到MNN)和Torchscripts；后者通过算子融合、算子替代、布局调整等方式优化图，一般离线运行。
- MNN-Compress: 模型压缩工具，在一定的精度误差许可下，对MNN模型进行压缩，减少模型体积，提升运行性能。
- MNN-Express ：支持带控制流的模型运行，支持调用 MNN 的算子进行自定义的计算。
- MNN-CV ：类似 OpenCV ，但核心计算功能基于 MNN 实现的图像处理算法库
- MNN-Train ：MNN 训练模块，支持各平台训练


## 1.4. 算子
在深度学习领域，算子（Operator），有时也被称为操作或层（Layer），是指神经网络中执行特定计算任务的基本单元。这些算子定义了数据如何在神经网络的不同层之间进行转换和传递。每个算子都负责执行一种特定类型的数学运算或处理步骤，如卷积、池化、激活函数应用等。通过组合不同的算子，可以构建出复杂而强大的深度学习模型，用于解决各种机器学习问题，比如图像识别、自然语言处理、语音识别等。

### 1.4.1. 常见的深度学习算子类型

1. **卷积算子（Convolution Operator）**：广泛应用于卷积神经网络（CNN）中，用于提取输入数据（如图像）中的特征。卷积算子通过一个称为滤波器或核的小窗口在输入数据上滑动，并在每个位置上执行元素级乘法和求和操作来生成输出。

2. **池化算子（Pooling Operator）**：通常与卷积算子一起使用，用于降低特征图的空间尺寸，减少参数数量，以及控制过拟合。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。

3. **激活函数算子（Activation Function Operator）**：引入非线性因素到模型中，使得神经网络能够学习更复杂的模式。常用的激活函数包括ReLU（Rectified Linear Unit）、Sigmoid和Tanh等。

4. **全连接算子（Fully Connected Operator）**：在神经网络的最后几层中常见，其中每个神经元都与前一层的所有神经元相连。这种结构允许网络学习输入数据的全局特征表示。

5. **归一化算子（Normalization Operator）**：如批量归一化（Batch Normalization），通过对每一小批训练数据进行归一化处理，加速训练过程并有助于防止梯度消失或爆炸的问题。

6. **损失函数算子（Loss Function Operator）**：衡量模型预测值与真实标签之间的差异，是训练过程中优化算法试图最小化的指标。


## MNN-Doc 
这是 [MNN-Doc](https://mnn-docs.readthedocs.io/) 的文档，包括教程、API参考、示例代码等。

## 1.5. 参考 
- [MNN 官网](https://www.mnn.zone/)
- [github MNN开源地址](https://github.com/alibaba/MNN)
- [MNN 文档](https://mnn-docs.readthedocs.io/)